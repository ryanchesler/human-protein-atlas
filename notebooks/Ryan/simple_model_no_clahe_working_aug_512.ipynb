{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import h5py\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512\n",
    "SEED = 777\n",
    "THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "DIR = '../input/'\n",
    "hdf5_path = 'D:\\Human-Protein-Atlas-Image-Classification\\input\\proteins.h5'\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# train_dataset_info = []\n",
    "# for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_train, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "# train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "paths, labels = getTrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 100\n",
    "# batch_size = 32\n",
    "# indexes = keys[idx * batch_size : (idx+1) * batch_size]\n",
    "# with h5py.File(hdf5_path, \"r\") as f:\n",
    "#     X = f[\"photos\"][list(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from random import randint\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, channels = [], shuffle = False, use_cache = False, augmentor = False, use_hdf5 = False, val = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.channels = channels\n",
    "        self.augmentor = augmentor\n",
    "        self.use_hdf5 = use_hdf5\n",
    "        self.clahe = cv2.createCLAHE()\n",
    "        self.val = val\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], len(channels)))\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        # Generate data\n",
    "        indexes = list(indexes)\n",
    "        indexes.sort()\n",
    "        with h5py.File(hdf5_path, \"r\") as f:\n",
    "            X = f[\"photos\"][indexes]\n",
    "            y = f[\"labels\"][indexes]\n",
    "#         if self.use_cache == True:\n",
    "#             X = self.cache[indexes]\n",
    "#             for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "#                 image = self.__load_image(path)\n",
    "#                 self.is_cached[indexes[i]] = 1\n",
    "#                 self.cache[indexes[i]] = image\n",
    "#                 X[i] = image\n",
    "#         elif self.use_cache == False and self.use_hdf5 == False:\n",
    "#             for i, path in enumerate(paths):\n",
    "#                 X[i] = self.__load_image(path)\n",
    "        if self.augmentor == True:\n",
    "            for i, item in enumerate(X):\n",
    "                X[i] = self.augment(item)\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = self.paths\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        images = []\n",
    "        for channel in self.channels:\n",
    "            im = np.array(Image.open(path + '_' + channel + '.png'))\n",
    "            \n",
    "#             im = clahe.apply(im)\n",
    "            images.append(im)\n",
    "            \n",
    "        if len(self.channels) >= 2:\n",
    "            im = np.stack((\n",
    "                images\n",
    "            ), -1)\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "\n",
    "        else:\n",
    "            im = images[0]\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "            im = np.expand_dims(im, 2)\n",
    "        return im\n",
    "    def augment(self, image):\n",
    "        if randint(0,1) == 1:\n",
    "            augment_img = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Flipud(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "                        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-4, 4)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "\n",
    "            image_aug = augment_img.augment_image(image)\n",
    "            return image_aug\n",
    "        else:\n",
    "            return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = (512, 512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels = [\"red\", \"green\", \"blue\"]\n",
    "# for path in paths[0:10]:\n",
    "#     images = []\n",
    "#     for channel in channels:\n",
    "#         im = np.array(Image.open(path + '_' + channel + '.png'))\n",
    "# #         im = cv2.equalizeHist(im)\n",
    "#         clahe = cv2.createCLAHE()\n",
    "#         im = clahe.apply(im)\n",
    "# #         plt.imshow(im)\n",
    "#         images.append(im)\n",
    "\n",
    "#     if len(channels) >= 2:\n",
    "#         im = np.stack((\n",
    "#             images\n",
    "#         ), -1)\n",
    "#         im = cv2.resize(im, (SIZE,SIZE))\n",
    "#         im = np.divide(im, 255)\n",
    "        \n",
    "        \n",
    "#     else:\n",
    "#         im = images[0]\n",
    "#         im = cv2.resize(im, (SIZE,SIZE))\n",
    "#         im = np.divide(im, 255)\n",
    "#         im = np.expand_dims(im, 2)\n",
    "#     plt.imshow(augment(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class data_generator:\n",
    "    \n",
    "#     def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "#         assert shape[2] == 3\n",
    "#         while True:\n",
    "#             dataset_info = shuffle(dataset_info)\n",
    "#             for start in range(0, len(dataset_info), batch_size):\n",
    "#                 end = min(start + batch_size, len(dataset_info))\n",
    "#                 batch_images = []\n",
    "#                 X_train_batch = dataset_info[start:end]\n",
    "#                 batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "#                 for i in range(len(X_train_batch)):\n",
    "#                     image = data_generator.load_image(\n",
    "#                         X_train_batch[i]['path'], shape)   \n",
    "#                     if augument:\n",
    "#                         image = data_generator.augment(image)\n",
    "#                     batch_images.append(image/255.)\n",
    "#                     batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "#                 yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "#     def load_image(path, shape):\n",
    "#         image_red_ch = Image.open(path+'_red.png')\n",
    "#         image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "#         image_green_ch = Image.open(path+'_green.png')\n",
    "#         image_blue_ch = Image.open(path+'_blue.png')\n",
    "#         image = np.stack((\n",
    "#         np.array(image_red_ch), \n",
    "#         np.array(image_green_ch), \n",
    "#         np.array(image_blue_ch)), -1)\n",
    "#         image = cv2.resize(image, (shape[0], shape[1]))\n",
    "#         return image\n",
    "\n",
    "#     def augment(image):\n",
    "#         augment_img = iaa.Sequential([\n",
    "#             iaa.OneOf([\n",
    "#                 iaa.Affine(rotate=0),\n",
    "#                 iaa.Affine(rotate=90),\n",
    "#                 iaa.Affine(rotate=180),\n",
    "#                 iaa.Affine(rotate=270),\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#             ])], random_order=True)\n",
    "\n",
    "#         image_aug = augment_img.augment_image(image)\n",
    "#         return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D, MaxPooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out, channels):\n",
    "    input_tensor = Input(shape=(299,299,len(channels)))\n",
    "\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=(299,299,3)\n",
    "                            )\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = Conv2D(3, kernel_size=(1,1), activation='relu', padding = \"same\")(bn)\n",
    "    x = base_model(x)\n",
    "    bn = BatchNormalization()(x)\n",
    "    x = Conv2D(128, kernel_size=(1,1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "#     output = Dense(n_out, activation='sigmoid')(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import six\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    ")\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def _bn_relu(input):\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "    \"\"\"Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides,\n",
    "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                           strides=init_strides,\n",
    "                           padding=\"same\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "                                  strides=init_strides)(input)\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
    "                              strides=init_strides,\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
    "                                     strides=init_strides)(input)\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "        \"\"\"Builds a custom ResNet like architecture.\n",
    "        Args:\n",
    "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
    "            num_outputs: The number of outputs at final softmax layer\n",
    "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
    "                The original paper used basic_block for layers < 50\n",
    "            repetitions: Number of repetitions of various block units.\n",
    "                At each block unit, the number of filters are doubled and the input size is halved\n",
    "        Returns:\n",
    "            The keras `Model`.\n",
    "        \"\"\"\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "        # Classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
    "                                 strides=(1, 1))(block)\n",
    "        pool2 = Dropout(0.5)(pool2)\n",
    "        flatten1 = Flatten()(pool2)\n",
    "        flatten1 = Dropout(0.5)(flatten1)\n",
    "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
    "                      activation=\"sigmoid\")(flatten1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_101(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_152(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetBuilder.build_resnet_18((4, SIZE, SIZE), 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 12608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 64) 0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, 128, 64) 0           add_1[0][0]                      \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  73856       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 128)  8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 128)  0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 128)  147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 128)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  295168      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 256)  0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 256)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 512)  1180160     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 512)  2048        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 512)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 512)  131584      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 512)  2359808     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 512)  0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 512)  2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 512)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 512)  2359808     activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 512)  2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 512)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 512)  2359808     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 512)  0           add_7[0][0]                      \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 512)  2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 512)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 512)    0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 28)           14364       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,204,828\n",
      "Trainable params: 11,197,020\n",
      "Non-trainable params: 7,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, n_out, channels):\n",
    "    input_tensor = Input(shape=(input_shape[0], input_shape[1] ,len(channels)))\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = Conv2D(8, kernel_size=(3,3), activation='relu', padding = \"same\")(bn)\n",
    "    x = Conv2D(8, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "    x = Conv2D(16, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = Conv2D(16, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "    x = Conv2D(32, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = Conv2D(32, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu', padding = \"same\")(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), activation='relu', padding = \"valid\")(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), activation='relu', padding = \"valid\")(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "#     output = Dense(n_out, activation='sigmoid')(x)\n",
    "    output = Dense(n_out, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 32;VAL_RATIO = .1;DEBUG = False\n",
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "if DEBUG == True:  # use only small subset for debugging, Kaggle's RAM is limited\n",
    "    pathsTrain = paths[0:256]\n",
    "    labelsTrain = labels[0:256]\n",
    "    pathsVal = paths[lastTrainIndex:lastTrainIndex+256]\n",
    "    labelsVal = labels[lastTrainIndex:lastTrainIndex+256]\n",
    "    use_cache = True\n",
    "else:\n",
    "    pathsTrain = keys[:lastTrainIndex]\n",
    "    labelsTrain = keys[:lastTrainIndex]\n",
    "    pathsVal = keys[lastTrainIndex:]\n",
    "    labelsVal = keys[lastTrainIndex:]\n",
    "    use_cache = False\n",
    "\n",
    "use_cache = False\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache, augmentor = True, use_hdf5 = True, shuffle = False)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache, augmentor = False, use_hdf5 = True, shuffle = False, val = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 612 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[0.        , 0.        , 0.10196079, 0.        ],\n",
       "          [0.        , 0.        , 0.13725491, 0.01568628],\n",
       "          [0.        , 0.        , 0.10588235, 0.00784314],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.1254902 , 0.02352941],\n",
       "          [0.01568628, 0.        , 0.14117648, 0.00392157],\n",
       "          [0.01176471, 0.        , 0.09019608, 0.01568628]],\n",
       " \n",
       "         [[0.00784314, 0.        , 0.11372549, 0.01960784],\n",
       "          [0.00784314, 0.        , 0.08235294, 0.00392157],\n",
       "          [0.00392157, 0.        , 0.2       , 0.01960784],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.13725491, 0.        ],\n",
       "          [0.02352941, 0.        , 0.10588235, 0.00784314],\n",
       "          [0.01568628, 0.        , 0.04705882, 0.03137255]],\n",
       " \n",
       "         [[0.        , 0.        , 0.10196079, 0.        ],\n",
       "          [0.00784314, 0.        , 0.09019608, 0.        ],\n",
       "          [0.00392157, 0.        , 0.10980392, 0.        ],\n",
       "          ...,\n",
       "          [0.01176471, 0.        , 0.1764706 , 0.00784314],\n",
       "          [0.01176471, 0.        , 0.06666667, 0.        ],\n",
       "          [0.04705882, 0.        , 0.17254902, 0.00784314]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.03921569, 0.        , 0.08235294, 0.06666667],\n",
       "          [0.10196079, 0.        , 0.10980392, 0.03137255],\n",
       "          [0.0627451 , 0.        , 0.15686275, 0.07843138],\n",
       "          ...,\n",
       "          [0.05882353, 0.        , 0.3647059 , 0.02745098],\n",
       "          [0.15686275, 0.        , 0.21176471, 0.01176471],\n",
       "          [0.06666667, 0.        , 0.13725491, 0.02745098]],\n",
       " \n",
       "         [[0.05098039, 0.        , 0.05098039, 0.02352941],\n",
       "          [0.01960784, 0.        , 0.04313726, 0.11764706],\n",
       "          [0.02745098, 0.        , 0.15294118, 0.08235294],\n",
       "          ...,\n",
       "          [0.0627451 , 0.        , 0.23921569, 0.01568628],\n",
       "          [0.14509805, 0.        , 0.16862746, 0.00392157],\n",
       "          [0.07058824, 0.        , 0.13725491, 0.03137255]],\n",
       " \n",
       "         [[0.04705882, 0.        , 0.10588235, 0.02745098],\n",
       "          [0.01176471, 0.        , 0.07843138, 0.04313726],\n",
       "          [0.        , 0.        , 0.13333334, 0.06666667],\n",
       "          ...,\n",
       "          [0.11764706, 0.        , 0.14509805, 0.00392157],\n",
       "          [0.11764706, 0.        , 0.23137255, 0.        ],\n",
       "          [0.03137255, 0.        , 0.20392157, 0.01568628]]],\n",
       " \n",
       " \n",
       "        [[[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00784314, 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.00392157, 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.00784314, 0.        , 0.        , 0.00392157],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.01176471, 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.00392157, 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.00392157, 0.        , 0.01568628, 0.01176471],\n",
       "          [0.        , 0.        , 0.        , 0.00392157],\n",
       "          [0.        , 0.        , 0.00392157, 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.01176471, 0.        , 0.        , 0.01176471],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.00784314, 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.00392157],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.01568628],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.00392157, 0.        , 0.01176471, 0.        ],\n",
       "          [0.03529412, 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.00392157, 0.        , 0.        , 0.00392157],\n",
       "          [0.        , 0.        , 0.03921569, 0.        ],\n",
       "          [0.        , 0.        , 0.00392157, 0.00392157]],\n",
       " \n",
       "         [[0.        , 0.        , 0.0627451 , 0.        ],\n",
       "          [0.00784314, 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.07450981, 0.01960784],\n",
       "          ...,\n",
       "          [0.00784314, 0.        , 0.05882353, 0.        ],\n",
       "          [0.        , 0.00784314, 0.01176471, 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.00392157]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.01176471, 0.        , 0.01176471, 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.00784314, 0.01960784, 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.01176471, 0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.        , 0.        , 0.03529412, 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.05490196, 0.        , 0.01568628, 0.        ],\n",
       "          ...,\n",
       "          [0.03921569, 0.        , 0.01960784, 0.00392157],\n",
       "          [0.00392157, 0.        , 0.09411765, 0.        ],\n",
       "          [0.01960784, 0.        , 0.01960784, 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.01176471, 0.        ],\n",
       "          [0.00392157, 0.        , 0.09019608, 0.        ],\n",
       "          [0.01176471, 0.        , 0.00392157, 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.03921569, 0.        ],\n",
       "          [0.        , 0.        , 0.04705882, 0.        ],\n",
       "          [0.        , 0.        , 0.02745098, 0.00392157]],\n",
       " \n",
       "         [[0.        , 0.        , 0.1764706 , 0.        ],\n",
       "          [0.01568628, 0.        , 0.        , 0.        ],\n",
       "          [0.01568628, 0.        , 0.06666667, 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.00392157, 0.00392157],\n",
       "          [0.00784314, 0.        , 0.        , 0.        ],\n",
       "          [0.02745098, 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.01960784, 0.        , 0.01176471, 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.00784314],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.09411765, 0.        ],\n",
       "          [0.00784314, 0.        , 0.00392157, 0.00392157],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.00392157],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.01960784, 0.        ],\n",
       "          [0.        , 0.        , 0.02352941, 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.21176471, 0.        , 0.08235294, 0.4       ],\n",
       "          [0.1882353 , 0.        , 0.03137255, 0.00784314],\n",
       "          [0.22745098, 0.        , 0.05490196, 0.08627451],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.12156863, 0.        , 0.09019608, 0.01960784],\n",
       "          [0.21960784, 0.        , 0.00392157, 0.11372549],\n",
       "          [0.13725491, 0.        , 0.05490196, 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.12941177, 0.        , 0.03529412, 0.08235294],\n",
       "          [0.14117648, 0.        , 0.02352941, 0.13333334],\n",
       "          [0.16470589, 0.        , 0.01960784, 0.08627451],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.00784314, 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.10588235, 0.        , 0.04313726, 0.08235294],\n",
       "          [0.03137255, 0.        , 0.08235294, 0.16862746],\n",
       "          [0.07058824, 0.        , 0.06666667, 0.1882353 ],\n",
       "          ...,\n",
       "          [0.03921569, 0.        , 0.05882353, 0.05882353],\n",
       "          [0.03529412, 0.        , 0.02352941, 0.0627451 ],\n",
       "          [0.08235294, 0.        , 0.02352941, 0.02745098]],\n",
       " \n",
       "         [[0.3764706 , 0.        , 0.04705882, 0.21960784],\n",
       "          [0.10588235, 0.        , 0.03529412, 0.13725491],\n",
       "          [0.05882353, 0.        , 0.01960784, 0.16862746],\n",
       "          ...,\n",
       "          [0.03529412, 0.        , 0.03529412, 0.01960784],\n",
       "          [0.        , 0.        , 0.07058824, 0.03137255],\n",
       "          [0.00392157, 0.        , 0.02745098, 0.08235294]],\n",
       " \n",
       "         [[0.08627451, 0.        , 0.01176471, 0.03529412],\n",
       "          [0.23137255, 0.        , 0.03137255, 0.11764706],\n",
       "          [0.08235294, 0.        , 0.07058824, 0.2509804 ],\n",
       "          ...,\n",
       "          [0.04313726, 0.        , 0.1254902 , 0.01568628],\n",
       "          [0.02352941, 0.        , 0.09411765, 0.00784314],\n",
       "          [0.00784314, 0.        , 0.05490196, 0.        ]]]],\n",
       "       dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(vg.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/InceptionV3.h5', monitor='val_f1', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=10, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_f1\", \n",
    "                      mode=\"max\", \n",
    "                      patience=20)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "def KerasFocalLoss(target, input):\n",
    "    \n",
    "    gamma = 2.\n",
    "    input = tf.cast(input, tf.float32)\n",
    "    \n",
    "    max_val = K.clip(-input, 0, 1)\n",
    "    loss = input - input * target + max_val + K.log(K.exp(-max_val) + K.exp(-input - max_val))\n",
    "    invprobs = tf.log_sigmoid(-input * (target * 2.0 - 1.0))\n",
    "    loss = K.exp(invprobs * gamma) * loss\n",
    "    \n",
    "    return K.mean(K.sum(loss, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # warm up model\n",
    "import tensorflow as tf\n",
    "# # with tf.device('/cpu:0'):\n",
    "model = simple_model(\n",
    "    input_shape=(SIZE,SIZE,len(channels)), \n",
    "    n_out=28, channels = channels)\n",
    "\n",
    "# # for layer in model.layers:\n",
    "# #     layer.trainable = False\n",
    "# # model.layers[1].trainable = True\n",
    "# # model.layers[2].trainable = True\n",
    "# # model.layers[-1].trainable = True\n",
    "# # model.layers[-2].trainable = True\n",
    "# # model.layers[-3].trainable = True\n",
    "# # model.layers[-4].trainable = True\n",
    "# # model.layers[-5].trainable = True\n",
    "# # model.layers[-6].trainable = True\n",
    "\n",
    "# model.summary()\n",
    "# # model = multi_gpu_model(model, gpus = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 512, 512, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 512, 512, 4)       16        \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 512, 512, 8)       296       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 512, 512, 8)       584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 256, 256, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 256, 256, 16)      1168      \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 256, 256, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               2359552   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 28)                7196      \n",
      "=================================================================\n",
      "Total params: 3,547,132\n",
      "Trainable params: 3,547,124\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=['binary_accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "874/874 [==============================] - 731s 836ms/step - loss: 0.1856 - binary_accuracy: 0.9392 - f1: 0.0483 - val_loss: 0.1701 - val_binary_accuracy: 0.9412 - val_f1: 0.0534\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.05340, saving model to ../working/InceptionV3.h5\n",
      "Epoch 2/100\n",
      "874/874 [==============================] - 693s 793ms/step - loss: 0.1672 - binary_accuracy: 0.9444 - f1: 0.0652 - val_loss: 0.1595 - val_binary_accuracy: 0.9462 - val_f1: 0.0600\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.05340 to 0.06000, saving model to ../working/InceptionV3.h5\n",
      "Epoch 3/100\n",
      "874/874 [==============================] - 702s 803ms/step - loss: 0.1595 - binary_accuracy: 0.9472 - f1: 0.0850 - val_loss: 0.1530 - val_binary_accuracy: 0.9465 - val_f1: 0.1030\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.06000 to 0.10304, saving model to ../working/InceptionV3.h5\n",
      "Epoch 4/100\n",
      "874/874 [==============================] - 699s 800ms/step - loss: 0.1527 - binary_accuracy: 0.9485 - f1: 0.1109 - val_loss: 0.1465 - val_binary_accuracy: 0.9495 - val_f1: 0.1215\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.10304 to 0.12146, saving model to ../working/InceptionV3.h5\n",
      "Epoch 5/100\n",
      "874/874 [==============================] - 684s 783ms/step - loss: 0.1462 - binary_accuracy: 0.9503 - f1: 0.1336 - val_loss: 0.1382 - val_binary_accuracy: 0.9522 - val_f1: 0.1533\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.12146 to 0.15328, saving model to ../working/InceptionV3.h5\n",
      "Epoch 6/100\n",
      "874/874 [==============================] - 704s 805ms/step - loss: 0.1412 - binary_accuracy: 0.9521 - f1: 0.1510 - val_loss: 0.1354 - val_binary_accuracy: 0.9532 - val_f1: 0.1581\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.15328 to 0.15815, saving model to ../working/InceptionV3.h5\n",
      "Epoch 7/100\n",
      "874/874 [==============================] - 734s 840ms/step - loss: 0.1363 - binary_accuracy: 0.9536 - f1: 0.1720 - val_loss: 0.1295 - val_binary_accuracy: 0.9555 - val_f1: 0.1910\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.15815 to 0.19095, saving model to ../working/InceptionV3.h5\n",
      "Epoch 8/100\n",
      "874/874 [==============================] - 745s 853ms/step - loss: 0.1321 - binary_accuracy: 0.9551 - f1: 0.1869 - val_loss: 0.1267 - val_binary_accuracy: 0.9558 - val_f1: 0.1862\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.19095\n",
      "Epoch 9/100\n",
      "874/874 [==============================] - 758s 867ms/step - loss: 0.1288 - binary_accuracy: 0.9559 - f1: 0.1961 - val_loss: 0.1274 - val_binary_accuracy: 0.9549 - val_f1: 0.2116\n",
      "\n",
      "Epoch 00009: val_f1 improved from 0.19095 to 0.21158, saving model to ../working/InceptionV3.h5\n",
      "Epoch 10/100\n",
      "874/874 [==============================] - 754s 862ms/step - loss: 0.1260 - binary_accuracy: 0.9567 - f1: 0.2079 - val_loss: 0.1247 - val_binary_accuracy: 0.9558 - val_f1: 0.2178\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.21158 to 0.21778, saving model to ../working/InceptionV3.h5\n",
      "Epoch 11/100\n",
      "874/874 [==============================] - 753s 862ms/step - loss: 0.1233 - binary_accuracy: 0.9573 - f1: 0.2165 - val_loss: 0.1221 - val_binary_accuracy: 0.9570 - val_f1: 0.2282\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.21778 to 0.22823, saving model to ../working/InceptionV3.h5\n",
      "Epoch 12/100\n",
      "874/874 [==============================] - 727s 831ms/step - loss: 0.1214 - binary_accuracy: 0.9582 - f1: 0.2247 - val_loss: 0.1215 - val_binary_accuracy: 0.9569 - val_f1: 0.2301\n",
      "\n",
      "Epoch 00012: val_f1 improved from 0.22823 to 0.23008, saving model to ../working/InceptionV3.h5\n",
      "Epoch 13/100\n",
      "874/874 [==============================] - 731s 837ms/step - loss: 0.1195 - binary_accuracy: 0.9584 - f1: 0.2305 - val_loss: 0.1185 - val_binary_accuracy: 0.9579 - val_f1: 0.2326\n",
      "\n",
      "Epoch 00013: val_f1 improved from 0.23008 to 0.23258, saving model to ../working/InceptionV3.h5\n",
      "Epoch 14/100\n",
      "874/874 [==============================] - 722s 827ms/step - loss: 0.1187 - binary_accuracy: 0.9586 - f1: 0.2349 - val_loss: 0.1176 - val_binary_accuracy: 0.9587 - val_f1: 0.2430\n",
      "\n",
      "Epoch 00014: val_f1 improved from 0.23258 to 0.24300, saving model to ../working/InceptionV3.h5\n",
      "Epoch 15/100\n",
      "874/874 [==============================] - 729s 834ms/step - loss: 0.1173 - binary_accuracy: 0.9591 - f1: 0.2380 - val_loss: 0.1174 - val_binary_accuracy: 0.9586 - val_f1: 0.2467\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.24300 to 0.24666, saving model to ../working/InceptionV3.h5\n",
      "Epoch 16/100\n",
      "874/874 [==============================] - 749s 857ms/step - loss: 0.1159 - binary_accuracy: 0.9594 - f1: 0.2442 - val_loss: 0.1158 - val_binary_accuracy: 0.9590 - val_f1: 0.2487\n",
      "\n",
      "Epoch 00016: val_f1 improved from 0.24666 to 0.24870, saving model to ../working/InceptionV3.h5\n",
      "Epoch 17/100\n",
      "874/874 [==============================] - 756s 866ms/step - loss: 0.1149 - binary_accuracy: 0.9597 - f1: 0.2475 - val_loss: 0.1175 - val_binary_accuracy: 0.9586 - val_f1: 0.2370\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.24870\n",
      "Epoch 18/100\n",
      "874/874 [==============================] - 820s 938ms/step - loss: 0.1143 - binary_accuracy: 0.9599 - f1: 0.2512 - val_loss: 0.1145 - val_binary_accuracy: 0.9589 - val_f1: 0.2529\n",
      "\n",
      "Epoch 00018: val_f1 improved from 0.24870 to 0.25292, saving model to ../working/InceptionV3.h5\n",
      "Epoch 19/100\n",
      "874/874 [==============================] - 815s 932ms/step - loss: 0.1129 - binary_accuracy: 0.9602 - f1: 0.2555 - val_loss: 0.1154 - val_binary_accuracy: 0.9590 - val_f1: 0.2583\n",
      "\n",
      "Epoch 00019: val_f1 improved from 0.25292 to 0.25833, saving model to ../working/InceptionV3.h5\n",
      "Epoch 20/100\n",
      "874/874 [==============================] - 795s 909ms/step - loss: 0.1121 - binary_accuracy: 0.9605 - f1: 0.2607 - val_loss: 0.1160 - val_binary_accuracy: 0.9590 - val_f1: 0.2505\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.25833\n",
      "Epoch 21/100\n",
      "874/874 [==============================] - 770s 881ms/step - loss: 0.1116 - binary_accuracy: 0.9604 - f1: 0.2602 - val_loss: 0.1121 - val_binary_accuracy: 0.9607 - val_f1: 0.2630\n",
      "\n",
      "Epoch 00021: val_f1 improved from 0.25833 to 0.26302, saving model to ../working/InceptionV3.h5\n",
      "Epoch 22/100\n",
      "874/874 [==============================] - 755s 864ms/step - loss: 0.1101 - binary_accuracy: 0.9612 - f1: 0.2668 - val_loss: 0.1148 - val_binary_accuracy: 0.9595 - val_f1: 0.2504\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.26302\n",
      "Epoch 23/100\n",
      "874/874 [==============================] - 761s 871ms/step - loss: 0.1096 - binary_accuracy: 0.9612 - f1: 0.2702 - val_loss: 0.1120 - val_binary_accuracy: 0.9605 - val_f1: 0.2616\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.26302\n",
      "Epoch 24/100\n",
      "874/874 [==============================] - 756s 865ms/step - loss: 0.1087 - binary_accuracy: 0.9615 - f1: 0.2714 - val_loss: 0.1143 - val_binary_accuracy: 0.9608 - val_f1: 0.2661\n",
      "\n",
      "Epoch 00024: val_f1 improved from 0.26302 to 0.26611, saving model to ../working/InceptionV3.h5\n",
      "Epoch 25/100\n",
      "874/874 [==============================] - 768s 879ms/step - loss: 0.1079 - binary_accuracy: 0.9617 - f1: 0.2737 - val_loss: 0.1125 - val_binary_accuracy: 0.9601 - val_f1: 0.2696\n",
      "\n",
      "Epoch 00025: val_f1 improved from 0.26611 to 0.26956, saving model to ../working/InceptionV3.h5\n",
      "Epoch 26/100\n",
      "874/874 [==============================] - 760s 869ms/step - loss: 0.1071 - binary_accuracy: 0.9622 - f1: 0.2764 - val_loss: 0.1126 - val_binary_accuracy: 0.9606 - val_f1: 0.2657\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.26956\n",
      "Epoch 27/100\n",
      "874/874 [==============================] - 760s 870ms/step - loss: 0.1068 - binary_accuracy: 0.9624 - f1: 0.2804 - val_loss: 0.1141 - val_binary_accuracy: 0.9594 - val_f1: 0.2712\n",
      "\n",
      "Epoch 00027: val_f1 improved from 0.26956 to 0.27125, saving model to ../working/InceptionV3.h5\n",
      "Epoch 28/100\n",
      "874/874 [==============================] - 747s 855ms/step - loss: 0.1061 - binary_accuracy: 0.9625 - f1: 0.2801 - val_loss: 0.1130 - val_binary_accuracy: 0.9599 - val_f1: 0.2614\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.27125\n",
      "Epoch 29/100\n",
      "874/874 [==============================] - 707s 809ms/step - loss: 0.1053 - binary_accuracy: 0.9625 - f1: 0.2838 - val_loss: 0.1149 - val_binary_accuracy: 0.9586 - val_f1: 0.2529\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.27125\n",
      "Epoch 30/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.1046 - binary_accuracy: 0.9626 - f1: 0.2871 - val_loss: 0.1114 - val_binary_accuracy: 0.9613 - val_f1: 0.2712\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.27125\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874/874 [==============================] - 693s 793ms/step - loss: 0.1039 - binary_accuracy: 0.9628 - f1: 0.2882 - val_loss: 0.1124 - val_binary_accuracy: 0.9603 - val_f1: 0.2674\n",
      "\n",
      "Epoch 00031: val_f1 did not improve from 0.27125\n",
      "Epoch 32/100\n",
      "874/874 [==============================] - 701s 802ms/step - loss: 0.1035 - binary_accuracy: 0.9632 - f1: 0.2875 - val_loss: 0.1125 - val_binary_accuracy: 0.9603 - val_f1: 0.2734\n",
      "\n",
      "Epoch 00032: val_f1 improved from 0.27125 to 0.27344, saving model to ../working/InceptionV3.h5\n",
      "Epoch 33/100\n",
      "874/874 [==============================] - 690s 790ms/step - loss: 0.1036 - binary_accuracy: 0.9633 - f1: 0.2873 - val_loss: 0.1088 - val_binary_accuracy: 0.9617 - val_f1: 0.2730\n",
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.27344\n",
      "Epoch 34/100\n",
      "874/874 [==============================] - 697s 798ms/step - loss: 0.1023 - binary_accuracy: 0.9637 - f1: 0.2924 - val_loss: 0.1078 - val_binary_accuracy: 0.9618 - val_f1: 0.2737\n",
      "\n",
      "Epoch 00034: val_f1 improved from 0.27344 to 0.27365, saving model to ../working/InceptionV3.h5\n",
      "Epoch 35/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.1021 - binary_accuracy: 0.9636 - f1: 0.2918 - val_loss: 0.1096 - val_binary_accuracy: 0.9615 - val_f1: 0.2689\n",
      "\n",
      "Epoch 00035: val_f1 did not improve from 0.27365\n",
      "Epoch 36/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.1025 - binary_accuracy: 0.9634 - f1: 0.2925 - val_loss: 0.1097 - val_binary_accuracy: 0.9612 - val_f1: 0.2692\n",
      "\n",
      "Epoch 00036: val_f1 did not improve from 0.27365\n",
      "Epoch 37/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.1018 - binary_accuracy: 0.9638 - f1: 0.2936 - val_loss: 0.1088 - val_binary_accuracy: 0.9621 - val_f1: 0.2860\n",
      "\n",
      "Epoch 00037: val_f1 improved from 0.27365 to 0.28599, saving model to ../working/InceptionV3.h5\n",
      "Epoch 38/100\n",
      "874/874 [==============================] - 688s 787ms/step - loss: 0.1021 - binary_accuracy: 0.9639 - f1: 0.2940 - val_loss: 0.1084 - val_binary_accuracy: 0.9620 - val_f1: 0.2737\n",
      "\n",
      "Epoch 00038: val_f1 did not improve from 0.28599\n",
      "Epoch 39/100\n",
      "874/874 [==============================] - 691s 790ms/step - loss: 0.1018 - binary_accuracy: 0.9638 - f1: 0.2954 - val_loss: 0.1120 - val_binary_accuracy: 0.9599 - val_f1: 0.2732\n",
      "\n",
      "Epoch 00039: val_f1 did not improve from 0.28599\n",
      "Epoch 40/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.1024 - binary_accuracy: 0.9635 - f1: 0.2917 - val_loss: 0.1112 - val_binary_accuracy: 0.9606 - val_f1: 0.2776\n",
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.28599\n",
      "Epoch 41/100\n",
      "874/874 [==============================] - 700s 801ms/step - loss: 0.1015 - binary_accuracy: 0.9638 - f1: 0.2928 - val_loss: 0.1092 - val_binary_accuracy: 0.9616 - val_f1: 0.2794\n",
      "\n",
      "Epoch 00041: val_f1 did not improve from 0.28599\n",
      "Epoch 42/100\n",
      "874/874 [==============================] - 693s 793ms/step - loss: 0.1003 - binary_accuracy: 0.9641 - f1: 0.2980 - val_loss: 0.1091 - val_binary_accuracy: 0.9611 - val_f1: 0.2803\n",
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.28599\n",
      "Epoch 43/100\n",
      "874/874 [==============================] - 690s 790ms/step - loss: 0.1004 - binary_accuracy: 0.9641 - f1: 0.2989 - val_loss: 0.1089 - val_binary_accuracy: 0.9618 - val_f1: 0.2781\n",
      "\n",
      "Epoch 00043: val_f1 did not improve from 0.28599\n",
      "Epoch 44/100\n",
      "874/874 [==============================] - 688s 787ms/step - loss: 0.1001 - binary_accuracy: 0.9642 - f1: 0.2976 - val_loss: 0.1118 - val_binary_accuracy: 0.9611 - val_f1: 0.2631\n",
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.28599\n",
      "Epoch 45/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.1001 - binary_accuracy: 0.9641 - f1: 0.3000 - val_loss: 0.1062 - val_binary_accuracy: 0.9630 - val_f1: 0.2861\n",
      "\n",
      "Epoch 00045: val_f1 improved from 0.28599 to 0.28611, saving model to ../working/InceptionV3.h5\n",
      "Epoch 46/100\n",
      "874/874 [==============================] - 689s 788ms/step - loss: 0.0998 - binary_accuracy: 0.9643 - f1: 0.3003 - val_loss: 0.1094 - val_binary_accuracy: 0.9621 - val_f1: 0.2870\n",
      "\n",
      "Epoch 00046: val_f1 improved from 0.28611 to 0.28705, saving model to ../working/InceptionV3.h5\n",
      "Epoch 47/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0995 - binary_accuracy: 0.9644 - f1: 0.3035 - val_loss: 0.1090 - val_binary_accuracy: 0.9623 - val_f1: 0.2823\n",
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.28705\n",
      "Epoch 48/100\n",
      "874/874 [==============================] - 689s 789ms/step - loss: 0.0995 - binary_accuracy: 0.9644 - f1: 0.3013 - val_loss: 0.1100 - val_binary_accuracy: 0.9617 - val_f1: 0.2774\n",
      "\n",
      "Epoch 00048: val_f1 did not improve from 0.28705\n",
      "Epoch 49/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0982 - binary_accuracy: 0.9649 - f1: 0.3044 - val_loss: 0.1074 - val_binary_accuracy: 0.9621 - val_f1: 0.2758\n",
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.28705\n",
      "Epoch 50/100\n",
      "874/874 [==============================] - 690s 789ms/step - loss: 0.0983 - binary_accuracy: 0.9649 - f1: 0.3057 - val_loss: 0.1085 - val_binary_accuracy: 0.9616 - val_f1: 0.2734\n",
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.28705\n",
      "Epoch 51/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0996 - binary_accuracy: 0.9644 - f1: 0.3023 - val_loss: 0.1087 - val_binary_accuracy: 0.9623 - val_f1: 0.2768\n",
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.28705\n",
      "Epoch 52/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0986 - binary_accuracy: 0.9647 - f1: 0.3005 - val_loss: 0.1088 - val_binary_accuracy: 0.9617 - val_f1: 0.2831\n",
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.28705\n",
      "Epoch 53/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.0981 - binary_accuracy: 0.9648 - f1: 0.3051 - val_loss: 0.1070 - val_binary_accuracy: 0.9627 - val_f1: 0.2867\n",
      "\n",
      "Epoch 00053: val_f1 did not improve from 0.28705\n",
      "Epoch 54/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0982 - binary_accuracy: 0.9649 - f1: 0.3053 - val_loss: 0.1094 - val_binary_accuracy: 0.9616 - val_f1: 0.2788\n",
      "\n",
      "Epoch 00054: val_f1 did not improve from 0.28705\n",
      "Epoch 55/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0988 - binary_accuracy: 0.9648 - f1: 0.3021 - val_loss: 0.1071 - val_binary_accuracy: 0.9630 - val_f1: 0.2798\n",
      "\n",
      "Epoch 00055: val_f1 did not improve from 0.28705\n",
      "Epoch 56/100\n",
      "874/874 [==============================] - 696s 796ms/step - loss: 0.0980 - binary_accuracy: 0.9650 - f1: 0.3079 - val_loss: 0.1070 - val_binary_accuracy: 0.9625 - val_f1: 0.2898\n",
      "\n",
      "Epoch 00056: val_f1 improved from 0.28705 to 0.28978, saving model to ../working/InceptionV3.h5\n",
      "Epoch 57/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0981 - binary_accuracy: 0.9649 - f1: 0.3047 - val_loss: 0.1100 - val_binary_accuracy: 0.9619 - val_f1: 0.2799\n",
      "\n",
      "Epoch 00057: val_f1 did not improve from 0.28978\n",
      "Epoch 58/100\n",
      "874/874 [==============================] - 691s 791ms/step - loss: 0.0968 - binary_accuracy: 0.9652 - f1: 0.3077 - val_loss: 0.1094 - val_binary_accuracy: 0.9618 - val_f1: 0.2756\n",
      "\n",
      "Epoch 00058: val_f1 did not improve from 0.28978\n",
      "Epoch 59/100\n",
      "874/874 [==============================] - 685s 783ms/step - loss: 0.0969 - binary_accuracy: 0.9652 - f1: 0.3105 - val_loss: 0.1078 - val_binary_accuracy: 0.9616 - val_f1: 0.2879\n",
      "\n",
      "Epoch 00059: val_f1 did not improve from 0.28978\n",
      "Epoch 60/100\n",
      "874/874 [==============================] - 696s 797ms/step - loss: 0.0976 - binary_accuracy: 0.9650 - f1: 0.3065 - val_loss: 0.1101 - val_binary_accuracy: 0.9617 - val_f1: 0.2783\n",
      "\n",
      "Epoch 00060: val_f1 did not improve from 0.28978\n",
      "Epoch 61/100\n",
      "874/874 [==============================] - 688s 787ms/step - loss: 0.0971 - binary_accuracy: 0.9651 - f1: 0.3099 - val_loss: 0.1103 - val_binary_accuracy: 0.9606 - val_f1: 0.2674\n",
      "\n",
      "Epoch 00061: val_f1 did not improve from 0.28978\n",
      "Epoch 62/100\n",
      "874/874 [==============================] - 689s 788ms/step - loss: 0.0967 - binary_accuracy: 0.9653 - f1: 0.3093 - val_loss: 0.1096 - val_binary_accuracy: 0.9618 - val_f1: 0.2786\n",
      "\n",
      "Epoch 00062: val_f1 did not improve from 0.28978\n",
      "Epoch 63/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0971 - binary_accuracy: 0.9652 - f1: 0.3091 - val_loss: 0.1116 - val_binary_accuracy: 0.9612 - val_f1: 0.2741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00063: val_f1 did not improve from 0.28978\n",
      "Epoch 64/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0970 - binary_accuracy: 0.9653 - f1: 0.3091 - val_loss: 0.1081 - val_binary_accuracy: 0.9618 - val_f1: 0.2765\n",
      "\n",
      "Epoch 00064: val_f1 did not improve from 0.28978\n",
      "Epoch 65/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0960 - binary_accuracy: 0.9654 - f1: 0.3129 - val_loss: 0.1073 - val_binary_accuracy: 0.9625 - val_f1: 0.2853\n",
      "\n",
      "Epoch 00065: val_f1 did not improve from 0.28978\n",
      "Epoch 66/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0963 - binary_accuracy: 0.9654 - f1: 0.3101 - val_loss: 0.1077 - val_binary_accuracy: 0.9626 - val_f1: 0.2824\n",
      "\n",
      "Epoch 00066: val_f1 did not improve from 0.28978\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 67/100\n",
      "874/874 [==============================] - 695s 796ms/step - loss: 0.0896 - binary_accuracy: 0.9676 - f1: 0.3284 - val_loss: 0.1059 - val_binary_accuracy: 0.9632 - val_f1: 0.2893\n",
      "\n",
      "Epoch 00067: val_f1 did not improve from 0.28978\n",
      "Epoch 68/100\n",
      "874/874 [==============================] - 702s 803ms/step - loss: 0.0876 - binary_accuracy: 0.9685 - f1: 0.3334 - val_loss: 0.1045 - val_binary_accuracy: 0.9634 - val_f1: 0.2906\n",
      "\n",
      "Epoch 00068: val_f1 improved from 0.28978 to 0.29055, saving model to ../working/InceptionV3.h5\n",
      "Epoch 69/100\n",
      "874/874 [==============================] - 692s 791ms/step - loss: 0.0863 - binary_accuracy: 0.9687 - f1: 0.3374 - val_loss: 0.1059 - val_binary_accuracy: 0.9631 - val_f1: 0.2867\n",
      "\n",
      "Epoch 00069: val_f1 did not improve from 0.29055\n",
      "Epoch 70/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0861 - binary_accuracy: 0.9690 - f1: 0.3396 - val_loss: 0.1051 - val_binary_accuracy: 0.9636 - val_f1: 0.2917\n",
      "\n",
      "Epoch 00070: val_f1 improved from 0.29055 to 0.29171, saving model to ../working/InceptionV3.h5\n",
      "Epoch 71/100\n",
      "874/874 [==============================] - 685s 783ms/step - loss: 0.0842 - binary_accuracy: 0.9693 - f1: 0.3450 - val_loss: 0.1052 - val_binary_accuracy: 0.9635 - val_f1: 0.2924\n",
      "\n",
      "Epoch 00071: val_f1 improved from 0.29171 to 0.29239, saving model to ../working/InceptionV3.h5\n",
      "Epoch 72/100\n",
      "874/874 [==============================] - 691s 791ms/step - loss: 0.0837 - binary_accuracy: 0.9698 - f1: 0.3469 - val_loss: 0.1057 - val_binary_accuracy: 0.9640 - val_f1: 0.2905\n",
      "\n",
      "Epoch 00072: val_f1 did not improve from 0.29239\n",
      "Epoch 73/100\n",
      "874/874 [==============================] - 694s 794ms/step - loss: 0.0825 - binary_accuracy: 0.9701 - f1: 0.3498 - val_loss: 0.1070 - val_binary_accuracy: 0.9632 - val_f1: 0.2927\n",
      "\n",
      "Epoch 00073: val_f1 improved from 0.29239 to 0.29272, saving model to ../working/InceptionV3.h5\n",
      "Epoch 74/100\n",
      "874/874 [==============================] - 714s 817ms/step - loss: 0.0823 - binary_accuracy: 0.9702 - f1: 0.3476 - val_loss: 0.1055 - val_binary_accuracy: 0.9636 - val_f1: 0.2994\n",
      "\n",
      "Epoch 00074: val_f1 improved from 0.29272 to 0.29935, saving model to ../working/InceptionV3.h5\n",
      "Epoch 75/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.0818 - binary_accuracy: 0.9704 - f1: 0.3503 - val_loss: 0.1055 - val_binary_accuracy: 0.9638 - val_f1: 0.2956\n",
      "\n",
      "Epoch 00075: val_f1 did not improve from 0.29935\n",
      "Epoch 76/100\n",
      "874/874 [==============================] - 695s 795ms/step - loss: 0.0819 - binary_accuracy: 0.9702 - f1: 0.3524 - val_loss: 0.1067 - val_binary_accuracy: 0.9636 - val_f1: 0.2954\n",
      "\n",
      "Epoch 00076: val_f1 did not improve from 0.29935\n",
      "Epoch 77/100\n",
      "874/874 [==============================] - 698s 798ms/step - loss: 0.0808 - binary_accuracy: 0.9707 - f1: 0.3558 - val_loss: 0.1071 - val_binary_accuracy: 0.9635 - val_f1: 0.2957\n",
      "\n",
      "Epoch 00077: val_f1 did not improve from 0.29935\n",
      "Epoch 78/100\n",
      "874/874 [==============================] - 696s 796ms/step - loss: 0.0810 - binary_accuracy: 0.9709 - f1: 0.3528 - val_loss: 0.1067 - val_binary_accuracy: 0.9638 - val_f1: 0.2984\n",
      "\n",
      "Epoch 00078: val_f1 did not improve from 0.29935\n",
      "Epoch 79/100\n",
      "874/874 [==============================] - 698s 799ms/step - loss: 0.0801 - binary_accuracy: 0.9708 - f1: 0.3557 - val_loss: 0.1075 - val_binary_accuracy: 0.9631 - val_f1: 0.2884\n",
      "\n",
      "Epoch 00079: val_f1 did not improve from 0.29935\n",
      "Epoch 80/100\n",
      "874/874 [==============================] - 692s 792ms/step - loss: 0.0797 - binary_accuracy: 0.9710 - f1: 0.3587 - val_loss: 0.1059 - val_binary_accuracy: 0.9641 - val_f1: 0.2979\n",
      "\n",
      "Epoch 00080: val_f1 did not improve from 0.29935\n",
      "Epoch 81/100\n",
      "874/874 [==============================] - 697s 797ms/step - loss: 0.0794 - binary_accuracy: 0.9710 - f1: 0.3593 - val_loss: 0.1056 - val_binary_accuracy: 0.9637 - val_f1: 0.2968\n",
      "\n",
      "Epoch 00081: val_f1 did not improve from 0.29935\n",
      "Epoch 82/100\n",
      "874/874 [==============================] - 698s 798ms/step - loss: 0.0788 - binary_accuracy: 0.9713 - f1: 0.3591 - val_loss: 0.1054 - val_binary_accuracy: 0.9646 - val_f1: 0.3030\n",
      "\n",
      "Epoch 00082: val_f1 improved from 0.29935 to 0.30304, saving model to ../working/InceptionV3.h5\n",
      "Epoch 83/100\n",
      "874/874 [==============================] - 692s 791ms/step - loss: 0.0785 - binary_accuracy: 0.9716 - f1: 0.3621 - val_loss: 0.1060 - val_binary_accuracy: 0.9643 - val_f1: 0.2905\n",
      "\n",
      "Epoch 00083: val_f1 did not improve from 0.30304\n",
      "Epoch 84/100\n",
      "874/874 [==============================] - 693s 793ms/step - loss: 0.0777 - binary_accuracy: 0.9718 - f1: 0.3645 - val_loss: 0.1061 - val_binary_accuracy: 0.9637 - val_f1: 0.2948\n",
      "\n",
      "Epoch 00084: val_f1 did not improve from 0.30304\n",
      "Epoch 85/100\n",
      "874/874 [==============================] - 698s 798ms/step - loss: 0.0776 - binary_accuracy: 0.9718 - f1: 0.3652 - val_loss: 0.1046 - val_binary_accuracy: 0.9641 - val_f1: 0.2931\n",
      "\n",
      "Epoch 00085: val_f1 did not improve from 0.30304\n",
      "Epoch 86/100\n",
      "874/874 [==============================] - 702s 804ms/step - loss: 0.0775 - binary_accuracy: 0.9720 - f1: 0.3639 - val_loss: 0.1062 - val_binary_accuracy: 0.9638 - val_f1: 0.2926\n",
      "\n",
      "Epoch 00086: val_f1 did not improve from 0.30304\n",
      "Epoch 87/100\n",
      "431/874 [=============>................] - ETA: 5:08 - loss: 0.0762 - binary_accuracy: 0.9720 - f1: 0.3685"
     ]
    }
   ],
   "source": [
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size)),\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size)),\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks = callbacks_list,\n",
    "        max_queue_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all layers\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "model.compile(loss=focal_loss(),\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(uid, i)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \"\"\"\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-466c7a451453>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-322f0ce0c041>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six-1.11.0-py3.6.egg\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=200, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=['accuracy', f1])\n",
    "# hist =  model.fit_generator(\n",
    "#         tg,\n",
    "#         steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "#         validation_data=vg,\n",
    "#         validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "#         epochs=200, \n",
    "#         verbose=1,\n",
    "#         callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "bestModel = load_model('../working/InceptionV3.h5', custom_objects={'f1': f1, 'f1_loss': f1_loss, 'focal_loss_fixed':focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lastFullValPred = np.empty((0, 28))\n",
    "lastFullValLabels = np.empty((0, 28))\n",
    "for i in tqdm(range(len(vg))): \n",
    "    im, lbl = vg[i]\n",
    "    scores = bestModel.predict(im)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "rng = np.arange(0, 1, 0.001)\n",
    "f1s = np.zeros((rng.shape[0], 28))\n",
    "for j,t in enumerate(tqdm(rng)):\n",
    "    for i in range(28):\n",
    "        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "        f1s[j,i] = scoref1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Individual F1-scores for each class:')\n",
    "print(np.max(f1s, axis=0))\n",
    "print('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))\n",
    "plt.plot(rng, f1s)\n",
    "T = np.empty(28)\n",
    "for i in range(28):\n",
    "    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "print('Probability threshold maximizing CV F1-score for each class:')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE, channels)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = bestModel.predict(images)\n",
    "    P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('transfer_1x1conv_aug_focal_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "# testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE)\n",
    "# submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "# P = np.zeros((pathsTest.shape[0], 28))\n",
    "# for i in tqdm(range(len(testg))):\n",
    "#     images, labels = testg[i]\n",
    "#     score = bestModel.predict(images)\n",
    "#     P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = []\n",
    "\n",
    "# for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "#     str_label = ''\n",
    "    \n",
    "#     for col in range(PP.shape[1]):\n",
    "#         if(PP[row, col] < .2):   # to account for losing TP is more costly than decreasing FP\n",
    "#             #print(PP[row])\n",
    "#             str_label += ''\n",
    "#         else:\n",
    "#             str_label += str(col) + ' '\n",
    "#     prediction.append(str_label.strip())\n",
    "    \n",
    "# submit['Predicted'] = np.array(prediction)\n",
    "# submit.to_csv('datagenerator_model_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
