{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUInceptionV3-TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "najlB4tnapzz",
        "colab_type": "code",
        "outputId": "b36ed340-29ff-42b9-c7f7-7863f6b89baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install imgaug"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.11.0)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.1.2)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.0.1)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (4.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2018.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image>=0.11.0->imgaug) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nReb0U3ydD9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/tensorflow/tpu.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4wTFMNrAZY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/competitions/human-protein-atlas-image-classification\n",
        "import os\n",
        "os.chdir('/content/competitions/human-protein-atlas-image-classification')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wLj6ncNParT3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RVBQEkhE1wFO",
        "colab_type": "code",
        "outputId": "78c4b09f-bdd2-40eb-f3f4-62307d88d8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.34.243.202:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 8618574030482350614),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3928385171969579737),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 6845812501833289711),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9107294441014786833),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12268605441063146510),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3795026296367617661),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12185785702808323202),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1552312615991546284),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14007639297446105232),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10183330090279571874),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6997512598918552462),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 469632495532780806)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5OO_GRS5PdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from skimage.transform import resize\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SIZE = 299\n",
        "SEED = 777\n",
        "THRESHOLD = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TopFylv55Pde",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAu2PQ_PD_fX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GCS access helpers ##\n",
        "Courtesy of https://stackoverflow.com/a/52106361/7724174\n",
        "\n",
        "These functions let us get data from GCS into our notebook."
      ]
    },
    {
      "metadata": {
        "id": "bDZlL-_K5Pdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load dataset info\n",
        "#DIR = '../input/'\n",
        "#DIR='gs://human-protein-atlas-kaggle/'\n",
        "#data = dd.read_csv(DIR+'train.csv')\n",
        "#data = data.compute()\n",
        "\n",
        "DATA_DIR='gs://human-protein-atlas-kaggle/'\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "with file_io.FileIO(DATA_DIR+'train.csv', 'r') as f:\n",
        "    data = pd.read_csv(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRMpE3B95Pdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SHAPE = (299, 299, 3)\n",
        "NUM_CLASSES=28\n",
        "#epochs = 400;\n",
        "epochs = 30\n",
        "#batch_size = 256;\n",
        "VAL_RATIO = .1;\n",
        "DEBUG = False\n",
        "channels = [\"green\", \"blue\", \"red\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ciXtnu0dfPO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data input pipline ##\n",
        "This isn't fully optimized yet, but it's good enough."
      ]
    },
    {
      "metadata": {
        "id": "kAm-HbRHN-U3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TF_DIR=DATA_DIR+'train'\n",
        "DS_DIMS=[512,512]\n",
        "NN_DIMS=[299,299]\n",
        "REC_BUF_SIZE=453762 # This is approximate size for 512x512 images\n",
        "NUM_PARALLEL_CALLS=2 # number of cores in the system\n",
        "class HPADataset:\n",
        "    def __init__(self, shards, aug=True):\n",
        "        self.shards = shards\n",
        "        self.aug = aug\n",
        "    def input_fn(self, params):\n",
        "        batch_size=params['batch_size']\n",
        "        def _parse_function(example_proto):\n",
        "            features = {}\n",
        "            for c in channels:\n",
        "                features[\"image/%s/filename\"%c] = tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "                features[\"image/%s/encoded\"%c] = tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "            features[\"image/label\"] = tf.FixedLenFeature((NUM_CLASSES), tf.float32, default_value=[0]*NUM_CLASSES)\n",
        "            parsed_features = tf.parse_single_example(example_proto, features)\n",
        "            imgs=[]\n",
        "            for c in channels:\n",
        "                img=parsed_features['image/%s/encoded'%c]\n",
        "                print(img)\n",
        "                img=tf.image.decode_png(img, channels=1)\n",
        "                shape=tf.shape(img)\n",
        "                shape_print=tf.print(shape)\n",
        "                img=tf.reshape(img, DS_DIMS)\n",
        "                imgs.append(img)\n",
        "            image=tf.stack(imgs, axis=-1, name='combine_channels')\n",
        "            image=tf.image.resize_images(image, NN_DIMS)\n",
        "            # For simplicity, we'll use imgaug with py_op here\n",
        "            def augment(image):\n",
        "                augment_img = iaa.Sequential([\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Affine(rotate=0),\n",
        "                        iaa.Affine(rotate=90),\n",
        "                        iaa.Affine(rotate=180),\n",
        "                        iaa.Affine(rotate=270),\n",
        "                        iaa.Fliplr(0.5),\n",
        "                        iaa.Flipud(0.5),\n",
        "                    ])], random_order=True)\n",
        "\n",
        "                image_aug = augment_img.augment_image(image)\n",
        "                return image_aug\n",
        "            if self.aug:\n",
        "                image=tf.py_func(augment, [image], tf.float32, name='augment')\n",
        "                image=tf.reshape(image, NN_DIMS+[len(channels)])\n",
        "            image=tf.cast(image, tf.float32)\n",
        "            image=image / 255.\n",
        "            return image, parsed_features[\"image/label\"]\n",
        "        fnames=['{dir}/hpa_{w}x{h}_{num}.tfrecords'.format(dir=TF_DIR, w=DS_DIMS[0], h=DS_DIMS[1], num=shard) for shard in self.shards]\n",
        "        dataset=tf.data.TFRecordDataset(fnames,\n",
        "                                        buffer_size=REC_BUF_SIZE*2*len(self.shards),\n",
        "                                        num_parallel_reads=len(self.shards))\n",
        "        dataset=dataset.map(_parse_function, num_parallel_calls=NUM_PARALLEL_CALLS)\n",
        "        dataset=dataset.shuffle(1000)\n",
        "        dataset=dataset.prefetch(batch_size*8)\n",
        "        dataset=dataset.batch(batch_size, drop_remainder=True)\n",
        "        dataset=dataset.prefetch(2)\n",
        "        dataset=dataset.repeat()\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "#with tf.Graph().as_default():\n",
        "#    test=HPADataset([1,2]).input_fn()\n",
        "#    with tf.Session() as sess:\n",
        "#        sess.run(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qX02e8aPkb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_VNMbwOsvV-4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tg = HPADataset(range(8), False)\n",
        "vg = HPADataset([8, 9], False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoxbV4Hsd1jA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model ##\n",
        "\n",
        "What follows is our model based on pure tensorflow and the inception model present there. Large portions of this code are lifted from https://github.com/tensorflow/tpu/blob/master/models/experimental/inception/inception_v3.py\n"
      ]
    },
    {
      "metadata": {
        "id": "kD7NGt2Y5Pdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import summary\n",
        "from tensorflow.contrib.framework.python.ops import arg_scope\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "from tensorflow.contrib.training.python.training import evaluation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k3QEuM7qf4rp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Some model settings\n",
        "precision='float32'\n",
        "log_device_placement=False\n",
        "clear_update_collections=True\n",
        "num_classes=NUM_CLASSES\n",
        "display_tensors=True\n",
        "use_tpu=True\n",
        "train_batch_size=1024\n",
        "geval_batch_size=1024\n",
        "glearning_rate=0.165\n",
        "learning_rate_decay=0.94\n",
        "use_learning_rate_warmup=False\n",
        "warmup_epochs=7\n",
        "cold_epochs=2\n",
        "learning_rate_decay_epochs=6\n",
        "skip_host_call=True\n",
        "goptimizer='RMS'\n",
        "moving_average=True\n",
        "MOVING_AVERAGE_DECAY = 0.995\n",
        "# Batchnorm moving mean/variance parameters\n",
        "BATCH_NORM_DECAY = 0.996\n",
        "BATCH_NORM_EPSILON = 1e-3\n",
        "\n",
        "WEIGHT_DECAY = 0.00004\n",
        "RMSPROP_DECAY = 0.9                # Decay term for RMSProp.\n",
        "RMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\n",
        "RMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n",
        "\n",
        "\n",
        "transpose_enabled=False\n",
        "\n",
        "_NUM_TRAIN_IMAGES = 24858\n",
        "_NUM_EVAL_IMAGES = 6214\n",
        "epochs=30\n",
        "ITERATIONS=_NUM_TRAIN_IMAGES*epochs/train_batch_size\n",
        "num_shards=8 # 8 in original..\n",
        "#model_dir='/content/competitions/human-protein-atlas-image-classification/output'\n",
        "model_dir=DATA_DIR+'output/'\n",
        "save_checkpoints_secs=1000\n",
        "save_summary_steps=100\n",
        "eval_timeout=None\n",
        "train_steps_per_eval=int(_NUM_TRAIN_IMAGES/train_batch_size)\n",
        "train_steps=int(ITERATIONS)\n",
        "print('Will train for {train} steps'.format(train=train_steps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9NYAVMEH45rM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def tensor_transform_fn(data, perm):\n",
        "  \"\"\"Transpose function.\n",
        "\n",
        "  This function is used to transpose an image tensor on the host and then\n",
        "  perform an inverse transpose on the TPU. The transpose on the TPU gets\n",
        "  effectively elided thus voiding any associated computational cost.\n",
        "\n",
        "  NOTE: Eventually the compiler will be able to detect when this kind of\n",
        "  operation may prove beneficial and perform these types of transformations\n",
        "  implicitly, voiding the need for user intervention\n",
        "\n",
        "  Args:\n",
        "    data: Tensor to be transposed\n",
        "    perm: New ordering of dimensions\n",
        "\n",
        "  Returns:\n",
        "    Transposed tensor\n",
        "  \"\"\"\n",
        "  if transpose_enabled:\n",
        "    return tf.transpose(data, perm)\n",
        "  return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Up9dhjFpe-14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inception_model_fn(features, labels, mode, params):\n",
        "    \"\"\"Inception v3 model using Estimator API.\"\"\"\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n",
        "\n",
        "    if isinstance(features, dict):\n",
        "        features = features['feature']\n",
        "\n",
        "    features = tensor_transform_fn(features, params['input_perm'])\n",
        "\n",
        "    # This nested function allows us to avoid duplicating the logic which\n",
        "    # builds the network, for different values of --precision.\n",
        "    def build_network(precision):\n",
        "        if precision == 'bfloat16':\n",
        "            with tf.contrib.tpu.bfloat16_scope():\n",
        "                logits, end_points = inception.inception_v3(\n",
        "                    features,\n",
        "                    num_classes,\n",
        "                    is_training=is_training)\n",
        "            logits = tf.cast(logits, tf.float32)\n",
        "        elif precision == 'float32':\n",
        "            logits, end_points = inception.inception_v3(\n",
        "                features,\n",
        "                num_classes,\n",
        "                is_training=is_training)\n",
        "        return logits, end_points\n",
        "\n",
        "    if clear_update_collections:\n",
        "        # updates_collections must be set to None in order to use fused batchnorm\n",
        "        with arg_scope(inception.inception_v3_arg_scope(\n",
        "            weight_decay=0.0,\n",
        "            batch_norm_decay=BATCH_NORM_DECAY,\n",
        "            batch_norm_epsilon=BATCH_NORM_EPSILON,\n",
        "            updates_collections=None)):\n",
        "            logits, end_points = build_network('float32')\n",
        "    else:\n",
        "        with arg_scope(inception.inception_v3_arg_scope(\n",
        "            batch_norm_decay=BATCH_NORM_DECAY,\n",
        "            batch_norm_epsilon=BATCH_NORM_EPSILON)):\n",
        "            logits, end_points = build_network('float32')\n",
        "\n",
        "    #### NEED TO CHECK HERE FOR MULTI-LABEL SUPPORT\n",
        "    predictions = {\n",
        "        'classes': tf.argmax(input=logits, axis=1),\n",
        "        'probabilities': tf.nn.sigmoid(logits, name='sigmoid_tensor')\n",
        "    }\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions=predictions,\n",
        "            export_outputs={\n",
        "                'classify': tf.estimator.export.PredictOutput(predictions)\n",
        "            })\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL and display_tensors and (\n",
        "        not use_tpu):\n",
        "        with tf.control_dependencies([\n",
        "            tf.Print(\n",
        "                predictions['classes'], [predictions['classes']],\n",
        "                summarize=geval_batch_size,\n",
        "                message='prediction: ')\n",
        "        ]):\n",
        "            labels = tf.Print(\n",
        "                labels, [labels], summarize=geval_batch_size, message='label: ')\n",
        "\n",
        "    # in our case labels come pre-encoded\n",
        "    one_hot_labels = labels #tf.one_hot(labels, num_classes, dtype=tf.int32)\n",
        "\n",
        "    if 'AuxLogits' in end_points:\n",
        "        tf.losses.softmax_cross_entropy(\n",
        "            onehot_labels=one_hot_labels,\n",
        "            logits=tf.cast(end_points['AuxLogits'], tf.float32),\n",
        "            weights=0.4,\n",
        "            label_smoothing=0.1,\n",
        "            scope='aux_loss')\n",
        "\n",
        "    tf.losses.sigmoid_cross_entropy(\n",
        "        multi_class_labels=one_hot_labels,\n",
        "        logits=logits,\n",
        "        weights=1.0,\n",
        "        label_smoothing=0.1)\n",
        "\n",
        "    losses = tf.add_n(tf.losses.get_losses())\n",
        "    l2_loss = []\n",
        "    for v in tf.trainable_variables():\n",
        "        if 'BatchNorm' not in v.name and 'weights' in v.name:\n",
        "            l2_loss.append(tf.nn.l2_loss(v))\n",
        "    loss = losses + WEIGHT_DECAY * tf.add_n(l2_loss)\n",
        "\n",
        "    initial_learning_rate = glearning_rate * train_batch_size / 256\n",
        "    if use_learning_rate_warmup:\n",
        "        # Adjust initial learning rate to match final warmup rate\n",
        "        warmup_decay = learning_rate_decay**(\n",
        "            (warmup_epochs + cold_epochs) /\n",
        "            learning_rate_decay_epochs)\n",
        "        adj_initial_learning_rate = initial_learning_rate * warmup_decay\n",
        "\n",
        "    final_learning_rate = 0.0001 * initial_learning_rate\n",
        "\n",
        "    host_call = None\n",
        "    train_op = None\n",
        "  \n",
        "    if is_training:\n",
        "        batches_per_epoch = _NUM_TRAIN_IMAGES / train_batch_size\n",
        "        global_step = tf.train.get_or_create_global_step()\n",
        "        current_epoch = tf.cast(\n",
        "            (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n",
        "\n",
        "        learning_rate = tf.train.exponential_decay(\n",
        "            learning_rate=initial_learning_rate,\n",
        "            global_step=global_step,\n",
        "            decay_steps=int(learning_rate_decay_epochs * batches_per_epoch),\n",
        "            decay_rate=learning_rate_decay,\n",
        "            staircase=True)\n",
        "\n",
        "        if use_learning_rate_warmup:\n",
        "            wlr = 0.1 * adj_initial_learning_rate\n",
        "            wlr_height = tf.cast(\n",
        "                0.9 * adj_initial_learning_rate /\n",
        "                (warmup_epochs + learning_rate_decay_epochs - 1),\n",
        "                tf.float32)\n",
        "            epoch_offset = tf.cast(cold_epochs - 1, tf.int32)\n",
        "            exp_decay_start = (warmup_epochs + cold_epochs +\n",
        "                             learning_rate_decay_epochs)\n",
        "            lin_inc_lr = tf.add(\n",
        "                wlr, tf.multiply(\n",
        "                    tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),\n",
        "                    wlr_height))\n",
        "            learning_rate = tf.where(\n",
        "                tf.greater_equal(current_epoch, cold_epochs),\n",
        "                (tf.where(tf.greater_equal(current_epoch, exp_decay_start),\n",
        "                          learning_rate, lin_inc_lr)),\n",
        "                wlr)\n",
        "\n",
        "        # Set a minimum boundary for the learning rate.\n",
        "        learning_rate = tf.maximum(\n",
        "            learning_rate, final_learning_rate, name='learning_rate')\n",
        "\n",
        "        if goptimizer == 'sgd':\n",
        "            tf.logging.info('Using SGD optimizer')\n",
        "            optimizer = tf.train.GradientDescentOptimizer(\n",
        "                learning_rate=learning_rate)\n",
        "        elif goptimizer == 'momentum':\n",
        "            tf.logging.info('Using Momentum optimizer')\n",
        "            optimizer = tf.train.MomentumOptimizer(\n",
        "                learning_rate=learning_rate, momentum=0.9)\n",
        "        elif goptimizer == 'RMS':\n",
        "            tf.logging.info('Using RMS optimizer')\n",
        "            optimizer = tf.train.RMSPropOptimizer(\n",
        "                learning_rate,\n",
        "                RMSPROP_DECAY,\n",
        "                momentum=RMSPROP_MOMENTUM,\n",
        "                epsilon=RMSPROP_EPSILON)\n",
        "        else:\n",
        "            tf.logging.fatal('Unknown optimizer:', optimizer)\n",
        "\n",
        "        if use_tpu:\n",
        "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "        if moving_average:\n",
        "            ema = tf.train.ExponentialMovingAverage(\n",
        "                decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n",
        "            variables_to_average = (\n",
        "                tf.trainable_variables() + tf.moving_average_variables())\n",
        "            with tf.control_dependencies([train_op]), tf.name_scope('moving_average'):\n",
        "                train_op = ema.apply(variables_to_average)\n",
        "\n",
        "        # To log the loss, current learning rate, and epoch for Tensorboard, the\n",
        "        # summary op needs to be run on the host CPU via host_call. host_call\n",
        "        # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n",
        "        # dimension. These Tensors are implicitly concatenated to\n",
        "        # [params['batch_size']].\n",
        "        gs_t = tf.reshape(global_step, [1])\n",
        "        loss_t = tf.reshape(loss, [1])\n",
        "        lr_t = tf.reshape(learning_rate, [1])\n",
        "        ce_t = tf.reshape(current_epoch, [1])\n",
        "\n",
        "        if not skip_host_call:\n",
        "            def host_call_fn(gs, loss, lr, ce):\n",
        "                \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "                This function is executed on the CPU and should not directly reference\n",
        "                any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "                model to the `metric_fn`, provide them as part of the `host_call`. See\n",
        "                https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "                for more information.\n",
        "                Arguments should match the list of `Tensor` objects passed as the second\n",
        "                element in the tuple passed to `host_call`.\n",
        "                Args:\n",
        "                  gs: `Tensor with shape `[batch]` for the global_step\n",
        "                  loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "                  lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "                  ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "                Returns:\n",
        "                  List of summary ops to run on the CPU host.\n",
        "                \"\"\"\n",
        "                gs = gs[0]\n",
        "                with summary.create_file_writer(model_dir).as_default():\n",
        "                    with summary.always_record_summaries():\n",
        "                        summary.scalar('loss', tf.reduce_mean(loss), step=gs)\n",
        "                        summary.scalar('learning_rate', tf.reduce_mean(lr), step=gs)\n",
        "                        summary.scalar('current_epoch', tf.reduce_mean(ce), step=gs)\n",
        "\n",
        "                    return summary.all_summary_ops()\n",
        "\n",
        "            host_call = (host_call_fn, [gs_t, loss_t, lr_t, ce_t])\n",
        "\n",
        "    eval_metrics = None\n",
        "    if is_eval:\n",
        "        def metric_fn(labels, logits):\n",
        "            \"\"\"Evaluation metric function. Evaluates accuracy.\n",
        "            This function is executed on the CPU and should not directly reference\n",
        "            any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n",
        "            to the `metric_fn`, provide as part of the `eval_metrics`. See\n",
        "            https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "            for more information.\n",
        "            Arguments should match the list of `Tensor` objects passed as the second\n",
        "            element in the tuple passed to `eval_metrics`.\n",
        "            Args:\n",
        "            labels: `Tensor` with shape `[batch, ]`.\n",
        "            logits: `Tensor` with shape `[batch, num_classes]`.\n",
        "            Returns:\n",
        "            A dict of the metrics to return from evaluation.\n",
        "            \"\"\"\n",
        "            probs=tf.nn.sigmoid(logits)\n",
        "            predictions = tf.math.greater(probs, 0.2)\n",
        "            recall = tf.metrics.recall(labels, predictions)\n",
        "            precision=tf.metrics.precision(labels, predictions)\n",
        "            f1=tf.contrib.metrics.f1_score(labels, probs)\n",
        "\n",
        "            return {\n",
        "              'recall': recall,\n",
        "              'precision': precision,\n",
        "              'f1': f1\n",
        "            }\n",
        "\n",
        "        eval_metrics = (metric_fn, [labels, logits])\n",
        "\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode,\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        host_call=host_call,\n",
        "        eval_metrics=eval_metrics)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fSZX5_Tv44y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LoadEMAHook(tf.train.SessionRunHook):\n",
        "  \"\"\"Hook to load exponential moving averages into corresponding variables.\"\"\"\n",
        "\n",
        "  def __init__(self, model_dir):\n",
        "    super(LoadEMAHook, self).__init__()\n",
        "    self._model_dir = model_dir\n",
        "\n",
        "  def begin(self):\n",
        "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
        "    variables_to_restore = ema.variables_to_restore()\n",
        "    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n",
        "        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n",
        "\n",
        "  def after_create_session(self, sess, coord):\n",
        "    tf.logging.info('Reloading EMA...')\n",
        "    self._load_ema(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atKLBN6BwJaZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def do_it(mode):\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  tf.logging.info('Precision: %s', precision)\n",
        "\n",
        "  params = {\n",
        "      'input_perm': [0, 1, 2, 3],\n",
        "      'output_perm': [0, 1, 2, 3],\n",
        "  }\n",
        "\n",
        "  batch_axis = 0\n",
        "  if transpose_enabled:\n",
        "    params['input_perm'] = [3, 0, 1, 2]\n",
        "    params['output_perm'] = [1, 2, 3, 0]\n",
        "    batch_axis = 3\n",
        "\n",
        "  eval_size = _NUM_EVAL_IMAGES\n",
        "  eval_steps = eval_size // geval_batch_size\n",
        "\n",
        "  iterations = (eval_steps if mode == 'eval' else save_summary_steps)\n",
        "\n",
        "  eval_batch_size = (None if mode == 'train' else geval_batch_size)\n",
        "\n",
        "  per_host_input_for_training = (num_shards <= 8 if mode == 'train' else True)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=model_dir,\n",
        "      save_checkpoints_secs=save_checkpoints_secs,\n",
        "      save_summary_steps=save_summary_steps,\n",
        "      session_config=tf.ConfigProto(\n",
        "          allow_soft_placement=True,\n",
        "          log_device_placement=log_device_placement),\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=iterations,\n",
        "          num_shards=num_shards,\n",
        "          per_host_input_for_training=per_host_input_for_training))\n",
        "\n",
        "  trainable_vars = tf.contrib.framework.get_model_variables()\n",
        "  print(trainable_vars)\n",
        "  skip_vars=['InceptionV3/AuxLogits/Conv2d_2b_1x1/weights']\n",
        "  load_vars = tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns=skip_vars)\n",
        "  print(load_vars)\n",
        "  ws = tf.estimator.WarmStartSettings(\n",
        "      ckpt_to_initialize_from=DATA_DIR+\"pre-trained/inception_v3.ckpt\",\n",
        "      vars_to_warm_start=load_vars\n",
        "  )\n",
        "  inception_classifier = tf.contrib.tpu.TPUEstimator(\n",
        "      model_fn=inception_model_fn,\n",
        "      use_tpu=use_tpu,\n",
        "      config=run_config,\n",
        "      warm_start_from=ws,\n",
        "      params=params,\n",
        "      train_batch_size=train_batch_size,\n",
        "      eval_batch_size=eval_batch_size,\n",
        "      batch_axis=(batch_axis, 0))\n",
        "\n",
        "  # Input pipelines are slightly different (with regards to shuffling and\n",
        "  # preprocessing) between training and evaluation.\n",
        "  use_bfloat16 = precision == 'bfloat16'\n",
        "  imagenet_train = tg\n",
        "  imagenet_eval = vg\n",
        "\n",
        "  if moving_average:\n",
        "    eval_hooks = [LoadEMAHook(model_dir)]\n",
        "  else:\n",
        "    eval_hooks = []\n",
        "\n",
        "  if mode == 'eval':\n",
        "    # Run evaluation when there is a new checkpoint\n",
        "    for checkpoint in evaluation.checkpoints_iterator(\n",
        "        model_dir, timeout=eval_timeout):\n",
        "      tf.logging.info('Starting to evaluate.')\n",
        "      try:\n",
        "        start_timestamp = time.time()  # Includes compilation time\n",
        "        eval_results = inception_classifier.evaluate(\n",
        "            input_fn=imagenet_eval.input_fn,\n",
        "            steps=eval_steps,\n",
        "            hooks=eval_hooks,\n",
        "            checkpoint_path=checkpoint)\n",
        "        elapsed_time = int(time.time() - start_timestamp)\n",
        "        tf.logging.info(\n",
        "            'Eval results: %s. Elapsed seconds: %d', eval_results, elapsed_time)\n",
        "\n",
        "        # Terminate eval job when final checkpoint is reached\n",
        "        current_step = int(os.path.basename(checkpoint).split('-')[1])\n",
        "        if current_step >= train_steps:\n",
        "          tf.logging.info(\n",
        "              'Evaluation finished after training step %d', current_step)\n",
        "          break\n",
        "      except tf.errors.NotFoundError:\n",
        "        # Since the coordinator is on a different job than the TPU worker,\n",
        "        # sometimes the TPU worker does not finish initializing until long after\n",
        "        # the CPU job tells it to start evaluating. In this case, the checkpoint\n",
        "        # file could have been deleted already.\n",
        "        tf.logging.info(\n",
        "            'Checkpoint %s no longer exists, skipping checkpoint', checkpoint)\n",
        "\n",
        "  elif mode == 'train_and_eval':\n",
        "    for cycle in range(train_steps // train_steps_per_eval):\n",
        "      tf.logging.info('Starting training cycle %d.' % cycle)\n",
        "      inception_classifier.train(\n",
        "          input_fn=imagenet_train.input_fn, steps=train_steps_per_eval)\n",
        "\n",
        "      tf.logging.info('Starting evaluation cycle %d .' % cycle)\n",
        "      eval_results = inception_classifier.evaluate(\n",
        "          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n",
        "      tf.logging.info('Evaluation results: %s' % eval_results)\n",
        "\n",
        "  else:\n",
        "    tf.logging.info('Starting training ...')\n",
        "    inception_classifier.train(\n",
        "        input_fn=imagenet_train.input_fn, max_steps=train_steps)\n",
        "\n",
        "  #if export_dir is not None:\n",
        "  #  tf.logging.info('Starting to export model.')\n",
        "  #  inception_classifier.export_saved_model(\n",
        "  #      export_dir_base=export_dir,\n",
        "  #      serving_input_receiver_fn=image_serving_input_fn)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrRGqcnH5Pem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2108
        },
        "outputId": "9e2045d8-de75-4591-cdea-62cf6a1c14d1"
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n",
        "do_it('train')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Precision: float32\n",
            "[]\n",
            "[]\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.57.37.130:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37ec9e0b38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.57.37.130:8470', '_evaluation_master': b'grpc://10.57.37.130:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f37ec9e08d0>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Starting training ...\n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.57.37.130:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 2940581086795091233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12029273475805367394)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 10207135047046682035)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12694888042175250773)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14725681623774298539)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 14543331399203177466)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15994278157382663187)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8015264919575951003)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14129410002919794916)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2572388507742196061)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6496484350152104165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5391750779753026532)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "Tensor(\"ParseSingleExample/ParseSingleExample:2\", shape=(), dtype=string, device=/job:worker/task:0/device:CPU:0)\n",
            "Tensor(\"ParseSingleExample/ParseSingleExample:0\", shape=(), dtype=string, device=/job:worker/task:0/device:CPU:0)\n",
            "Tensor(\"ParseSingleExample/ParseSingleExample:5\", shape=(), dtype=string, device=/job:worker/task:0/device:CPU:0)\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Using RMS optimizer\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt', vars_to_warm_start=[], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
            "INFO:tensorflow:Warm-starting from: ('gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt',)\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-0\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 8 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 3875441.5, step = 100\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 12811181.0, step = 200 (296.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.337748\n",
            "INFO:tensorflow:examples/sec: 345.854\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Saving checkpoints for 300 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:loss = 18639590.0, step = 300 (324.198 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.308453\n",
            "INFO:tensorflow:examples/sec: 315.856\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 33696724.0, step = 400 (274.337 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.364516\n",
            "INFO:tensorflow:examples/sec: 373.264\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 28558148.0, step = 500 (295.733 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.338143\n",
            "INFO:tensorflow:examples/sec: 346.259\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 33552680.0, step = 600 (294.975 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.339012\n",
            "INFO:tensorflow:examples/sec: 347.148\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Saving checkpoints for 700 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:loss = 36309850.0, step = 700 (331.707 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.301472\n",
            "INFO:tensorflow:examples/sec: 308.707\n",
            "INFO:tensorflow:Enqueue next (28) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (28) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 44150184.0, step = 728 (55.984 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 728 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 44150184.0.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TYN3etXr2NRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1424
        },
        "outputId": "f534737a-a961-48e3-ee4d-3ac283c21af6"
      },
      "cell_type": "code",
      "source": [
        "do_it('eval')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Precision: float32\n",
            "[]\n",
            "[]\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.57.37.130:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37eb4e50f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.57.37.130:8470', '_evaluation_master': b'grpc://10.57.37.130:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=6, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f37eb534390>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Waiting for new checkpoint at gs://human-protein-atlas-kaggle/output/\n",
            "INFO:tensorflow:Found new checkpoint at gs://human-protein-atlas-kaggle/output/model.ckpt-728\n",
            "INFO:tensorflow:Starting to evaluate.\n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.57.37.130:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 2940581086795091233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12029273475805367394)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 10207135047046682035)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12694888042175250773)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14725681623774298539)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 14543331399203177466)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15994278157382663187)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8015264919575951003)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14129410002919794916)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2572388507742196061)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6496484350152104165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5391750779753026532)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "Tensor(\"ParseSingleExample/ParseSingleExample:2\", shape=(), dtype=string, device=/job:worker/task:0/device:CPU:0)\n",
            "Tensor(\"ParseSingleExample/ParseSingleExample:0\", shape=(), dtype=string, device=/job:worker/task:0/device:CPU:0)\n",
            "Tensor(\"ParseSingleExample/ParseSingleExample:5\", shape=(), dtype=string, device=/job:worker/task:0/device:CPU:0)\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-11-09-17:21:55\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-728\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Reloading EMA...\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-728\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (6) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (6) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Evaluation [6/6]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2018-11-09-17:23:00\n",
            "INFO:tensorflow:Saving dict for global step 728: f1 = 0.37409037, global_step = 728, loss = 26454818.0, precision = 0.26801217, recall = 0.49217895\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 728: gs://human-protein-atlas-kaggle/output/model.ckpt-728\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "INFO:tensorflow:Eval results: {'f1': 0.37409037, 'loss': 26454818.0, 'precision': 0.26801217, 'recall': 0.49217895, 'global_step': 728}. Elapsed seconds: 73\n",
            "INFO:tensorflow:Evaluation finished after training step 728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NgCutQhuc1B9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}